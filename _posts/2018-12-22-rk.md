---
layout: post
title: Runge Kutta Optimizers
---

Last year, I started to move towards machine learning from mathematics. To do this I was in the
lookout for interesting research projects to work on.


Suppose we have a parametric model, $f_{\theta}$, and a loss function ${L}$. Then the most
commonly used optimization scheme in deep learning is stochastic gradient descent, which is based
on gradient descent. Gradient descent updates are defined as follows

$$
\theta_{t + 1} = \theta_{t} - \epsilon \nabla_{\theta} {L}(f_{\theta})
$$

where $\epsilon$ is referred to as the learning rate.

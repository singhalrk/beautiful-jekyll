---
layout: post
title: "The power of Stein's method in high-dimensions"
keywords: Stein's method, machine learning, generative modeling, bayesian inference
---

Computing the distance between two distributions is an important part of
machine learning and statisitcs. For example, if we have a target density
$\mathbb{P}$ with a density, $p(\mathbf{x}) \propto \exp(V(\mathbf{x}))$, known
upto a normalization constant, then we can use _MCMC_ to approximately sample
from $p$.

After running the _MCMC_ chain for a number of steps, we get a set of samples
$(\mathbf{x}^{i})_ {i=1}^N$. If the _MCMC_ chain is unbiased then we know the
sampling is asymptotically exact, that is

$$
Q_{n} \equiv \frac{1}{N}\sum _{i=1}^{N} \mathbf{x}^{i} \Rightarrow \mathbb{P}
$$

However, if the sampler is not asymptotically exact or we would like to see if
the chain is mixing, then we can measure the distance between $\mathbb{P}$ and
$Q_{n}$. There are several ways to measure the distance between $\mathbb{P},
Q_n$, we mention integral probability metrcs.

An integral probability metric (_IPM_), $D(\mathbb{P}, \mathbb{Q})$, is a
distance between two probability distributions $\mathbb{P}, \mathbb{Q}$. Given
a function class $\mathcal{F}$, we define an _IPM_ as follows

$$
D(\mathbb{P}, \mathbb{Q}) = \sup_{f \in \mathcal{F}}
\left| \E_{\mathbb{P}}f - \E_{\mathbb{Q}}f \right|
$$

If the function class consists of all functions $f: \mathbb{R}^d \rightarrow
\mathbb{R}$ with Lipschitz constant bounded by $1$, then $D$ is the
Wasserstein-1 distance. If $f$ is bounded by $1$, then $D$ is the total
variation metric.

<!-- however they require either samples from both, likelihoods from both or -->
<!-- some expensive optimzation procedure. We detail -->
